# Servidor RAG Personal con MCP

Este proyecto implementa un servidor compatible con el Protocolo de Contexto de Modelo (MCP) que dota a los clientes de IA (como Cursor, Claude for Desktop, etc.) de una capacidad de Recuperaci√≥n Aumentada por Generaci√≥n (RAG). Permite al modelo de lenguaje acceder a una base de conocimiento privada y local, alimentada por tus propios textos y documentos.

## ‚ú® Caracter√≠sticas

- **Memoria Persistente para tu IA:** "Ense√±a" a tu IA nueva informaci√≥n que recordar√° entre sesiones.
- **Procesamiento de Documentos:** Alimenta la base de conocimiento con archivos `.pdf`, `.docx`, `.pptx`, `.txt`, y m√°s, gracias a la integraci√≥n con [Microsoft MarkItDown](https://github.com/microsoft/markitdown).
- **LLM Local y Privado:** Utiliza modelos de lenguaje locales a trav√©s de [Ollama](https://ollama.com/) (ej. Llama 3, Mistral), asegurando que tus datos y preguntas nunca salgan de tu m√°quina.
- **100% Local y Offline:** Tanto el modelo de lenguaje como los embeddings se ejecutan en tu m√°quina. Ning√∫n dato sale a internet. Una vez descargados los modelos, funciona sin conexi√≥n.
- **Ingesta Masiva:** Un script dedicado para procesar directorios enteros de documentos y construir la base de conocimiento de manera eficiente.
- **Arquitectura Modular:** La l√≥gica del RAG est√° separada de los scripts de servidor y de ingesta, facilitando el mantenimiento y la expansi√≥n.
- **Copias en Markdown:** Cada documento procesado se guarda autom√°ticamente en formato Markdown para verificaci√≥n y reutilizaci√≥n.
- **üÜï Metadatos de Fuente:** Rastreabilidad completa de informaci√≥n con atribuci√≥n de fuentes en cada respuesta.
- **üÜï Optimizado para Agentes de IA:** Descripciones detalladas y manejo de errores inteligente para uso efectivo por agentes de IA.

---

## üèóÔ∏è Arquitectura

El proyecto est√° dividido en tres componentes principales:

1.  `rag_core.py`: El coraz√≥n del sistema. Contiene toda la l√≥gica reutilizable para manejar la base de datos vectorial (ChromaDB), procesar texto y crear la cadena de preguntas y respuestas con LangChain. **Incluye soporte para metadatos de fuente.**
2.  `rag_server.py`: El servidor MCP. Expone las herramientas (`learn_text`, `learn_document`, `ask_rag`) que el cliente de IA puede invocar. Se comunica a trav√©s de `stdio`. **Optimizado con descripciones detalladas para agentes de IA.**
3.  `bulk_ingest.py`: Un script de l√≠nea de comandos para procesar una carpeta llena de documentos y a√±adirlos a la base de conocimiento de forma masiva. **Incluye metadatos de fuente autom√°ticos.**

### Archivos de Documentaci√≥n:
- `AGENT_INSTRUCTIONS.md`: Gu√≠a completa para agentes de IA sobre c√≥mo usar el sistema
- `test_rag.py`: Script de prueba para verificar el funcionamiento del sistema

---

## üöÄ Gu√≠a de Instalaci√≥n y Configuraci√≥n

Sigue estos pasos para poner en marcha el sistema.

### Prerrequisitos

- **Python 3.10+**
- **Ollama:** Aseg√∫rate de que [Ollama est√© instalado](https://ollama.com/) y en ejecuci√≥n en tu sistema.

### 0. Configuraci√≥n de Ollama (Paso Cr√≠tico)

Ollama es necesario para que el sistema RAG funcione, ya que proporciona el modelo de lenguaje local que genera las respuestas.

#### Instalaci√≥n de Ollama

**Windows:**
1. Descarga Ollama desde [ollama.com](https://ollama.com/)
2. Ejecuta el instalador y sigue las instrucciones
3. Ollama se ejecutar√° autom√°ticamente como servicio

**macOS/Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

#### Verificar Instalaci√≥n

```bash
# Verificar que Ollama est√° funcionando
ollama --version

# Verificar que el servicio est√° ejecut√°ndose
ollama list
```

#### Descargar Modelos de Lenguaje

El sistema RAG necesita un modelo de lenguaje para generar respuestas. Recomendamos:

```bash
# Modelo recomendado (equilibrio entre velocidad y calidad)
ollama pull llama3

# Alternativas m√°s r√°pidas
ollama pull phi3
ollama pull mistral

# Alternativa m√°s potente (requiere m√°s recursos)
ollama pull llama3.1:8b
```

#### Configurar el Modelo en el Sistema

Una vez descargado el modelo, aseg√∫rate de que `rag_core.py` use el modelo correcto:

```python
# En rag_core.py, l√≠nea ~100, verifica que use tu modelo:
llm = ChatOllama(model="llama3", temperature=0)
```

**Nota:** Si descargaste un modelo diferente, cambia `"llama3"` por el nombre de tu modelo.

#### Probar Ollama

```bash
# Probar que el modelo funciona
ollama run llama3 "Hola, ¬øc√≥mo est√°s?"
```

Si ves una respuesta generada, Ollama est√° funcionando correctamente.

#### Soluci√≥n de Problemas Comunes

**Error: "Ollama is not running"**
```bash
# Iniciar Ollama manualmente
ollama serve
```

**Error: "Model not found"**
```bash
# Verificar modelos disponibles
ollama list

# Descargar el modelo si no est√°
ollama pull llama3
```

**Error: "Out of memory"**
- Usa un modelo m√°s peque√±o: `ollama pull phi3`
- Cierra otras aplicaciones que consuman mucha RAM
- Considera aumentar la memoria virtual en Windows

### 1. Configuraci√≥n del Entorno

```bash
# 1. Clona este repositorio (si estuviera en GitHub) o usa los archivos existentes.
# cd RAG_MCP_Project

# 2. Crea un entorno virtual de Python
python -m venv .venv

# 3. Activa el entorno virtual
# En Windows:
.venv\\Scripts\\activate
# En macOS/Linux:
# source .venv/bin/activate
```

### 2. Instalaci√≥n de Dependencias

Una vez que el entorno virtual est√© activado, instala todas las librer√≠as necesarias.

#### Opci√≥n A: Instalaci√≥n Completa (Recomendada)
```bash
pip install -r requirements.txt
```

#### Opci√≥n B: Instalaci√≥n M√≠nima (Para pruebas r√°pidas)
```bash
pip install -r requirements-minimal.txt
```

#### Opci√≥n C: Instalaci√≥n de Desarrollo (Para contribuir al proyecto)
```bash
pip install -r requirements-dev.txt
```

**Nota:** La instalaci√≥n completa incluye todas las dependencias necesarias. La instalaci√≥n m√≠nima omite algunas utilidades opcionales pero mantiene la funcionalidad core. La instalaci√≥n de desarrollo incluye herramientas de testing y desarrollo.

### 2. Verificaci√≥n Completa del Sistema

Antes de continuar, vamos a verificar que todo est√© funcionando correctamente:

#### Paso 1: Verificar Ollama
```bash
# Verificar que Ollama est√° ejecut√°ndose
ollama list

# Probar el modelo
ollama run llama3 "Test de funcionamiento"
```

#### Paso 2: Verificar Dependencias de Python
```bash
# Verificar que todas las dependencias est√°n instaladas
python -c "import mcp; print('‚úÖ MCP instalado correctamente')"
python -c "import langchain; print('‚úÖ LangChain instalado correctamente')"
python -c "import chromadb; print('‚úÖ ChromaDB instalado correctamente')"
```

#### Paso 3: Probar el Sistema RAG
```bash
# Ejecutar el script de prueba
python test_rag.py
```

Si todo funciona correctamente, ver√°s:
- ‚úÖ Ollama respondiendo a comandos
- ‚úÖ Todas las dependencias import√°ndose sin errores
- ‚úÖ El sistema RAG procesando preguntas y mostrando fuentes

### 3. Descarga del Modelo Local

**Nota:** Si ya descargaste el modelo en el paso 0, puedes saltar esta secci√≥n.

Abre una terminal y descarga el modelo de lenguaje que usar√° Ollama para generar las respuestas.

```bash
# Modelo recomendado para el sistema RAG
ollama pull llama3
```

**Alternativas de modelos:**

| Modelo | Tama√±o | Velocidad | Calidad | Uso Recomendado |
|--------|--------|-----------|---------|-----------------|
| `llama3` | ~4GB | Media | Alta | ‚úÖ **Recomendado** |
| `phi3` | ~2GB | R√°pida | Buena | Para recursos limitados |
| `mistral` | ~4GB | Media | Alta | Alternativa a llama3 |
| `llama3.1:8b` | ~5GB | Lenta | Muy alta | Para m√°xima calidad |

**Nota:** La primera vez que ejecutes el servidor o el script de ingesta, el modelo de *embedding* (`all-MiniLM-L6-v2`, unos 90MB) se descargar√° autom√°ticamente. Esto solo ocurre una vez.

**Verificar descarga:**
```bash
# Verificar que el modelo est√° disponible
ollama list

# Probar el modelo
ollama run llama3 "Hola, ¬øpuedes ayudarme con el sistema RAG?"
```

### 4. Configurar el Modelo en el C√≥digo

Si descargaste un modelo diferente a `llama3`, necesitas actualizar la configuraci√≥n:

#### Opci√≥n 1: Descargar el Modelo de Embedding (Recomendado la primera vez)
Para evitar esperas la primera vez que se usa el servidor, puedes pre-descargar el modelo de embedding con este comando. Ver√°s una barra de progreso:
```bash
python pre_download_model.py
```

#### Opci√≥n 2: Cambiar en rag_core.py
```python
# Abrir rag_core.py y buscar la l√≠nea ~100
# Cambiar esta l√≠nea:
llm = ChatOllama(model="llama3", temperature=0)

# Por tu modelo, por ejemplo:
llm = ChatOllama(model="phi3", temperature=0)
```

#### Opci√≥n 3: Usar Variable de Entorno (Recomendado)
Crea un archivo `.env` en la ra√≠z del proyecto:

```bash
# Crear archivo .env
echo "OLLAMA_MODEL=llama3" > .env
```

Y modifica `rag_core.py` para usar la variable de entorno:

```python
import os
from dotenv import load_dotenv

load_dotenv()
model_name = os.getenv("OLLAMA_MODEL", "llama3")  # Por defecto llama3
llm = ChatOllama(model=model_name, temperature=0)
```

**Ventajas de usar variable de entorno:**
- F√°cil cambio de modelo sin modificar c√≥digo
- Configuraci√≥n espec√≠fica por entorno
- No se modifica el c√≥digo fuente

---

## ‚úÖ Resumen de Configuraci√≥n

Para verificar que todo est√° listo, ejecuta esta secuencia de comandos:

```bash
# 1. Verificar Ollama
ollama list
ollama run llama3 "Test"

# 2. Verificar dependencias
python -c "import mcp, langchain, chromadb; print('‚úÖ Todas las dependencias OK')"

# 3. Probar el sistema completo
python test_rag.py
```

**Si todo funciona correctamente, ver√°s:**
- ‚úÖ Lista de modelos de Ollama
- ‚úÖ Respuesta del modelo de prueba
- ‚úÖ Todas las dependencias import√°ndose
- ‚úÖ Sistema RAG procesando preguntas con fuentes

**¬°Tu sistema RAG est√° listo para usar!** üöÄ

---

## üõ†Ô∏è Gu√≠a de Uso

### Uso 1: Poblar la Base de Conocimiento (Ingesta Masiva)

Para a√±adir una gran cantidad de documentos de una sola vez, usa el script `bulk_ingest.py`.

1.  Crea una carpeta en tu ordenador (ej. `C:\MisDocumentos`).
2.  Copia todos los documentos que quieres que la IA aprenda en esa carpeta.
3.  Ejecuta el siguiente comando en la terminal (con el entorno virtual activado):

```bash
python bulk_ingest.py --directory "C:\MisDocumentos"
```

El script recorrer√° todos los archivos soportados, los convertir√° y los a√±adir√° a la base de datos vectorial en la carpeta `./rag_mcp_db`.

### Uso 2: Configuraci√≥n del Cliente MCP (Ej. Cursor)

Para que tu editor de IA pueda usar el servidor, debes configurarlo.

1.  **Encuentra el archivo de configuraci√≥n de servidores MCP de tu editor.** Para Cursor, busca un archivo como `mcp_servers.json` en su directorio de configuraci√≥n (`%APPDATA%\cursor` en Windows). Si no existe, puedes crearlo.

2.  **A√±ade la siguiente configuraci√≥n al archivo JSON.**
    
    Este m√©todo utiliza un script de arranque (`run_server.bat`) para asegurar que la codificaci√≥n de caracteres sea UTF-8, previniendo errores en Windows.

    **¬°IMPORTANTE!** Debes reemplazar `"D:\\ruta\\completa\\a\\tu\\proyecto\\MCP_RAG"` con la ruta absoluta real a la carpeta de este proyecto en tu m√°quina.

    ```json
    {
      "mcpServers": {
        "rag_server_knowledge": {
          "command": "D:\\ruta\\completa\\a\\tu\\proyecto\\MCP_RAG\\run_server.bat",
          "args": [],
          "workingDirectory": "D:\\ruta\\completa\\a\\tu\\proyecto\\MCP_RAG"
        }
      }
    }
    ```

3.  **Reinicia tu editor.** Al arrancar, deber√≠a detectar y lanzar tu `run_server.bat`, que a su vez ejecutar√° `rag_server.py` en segundo plano con el entorno correcto.

### Uso 3: Interactuando con las Herramientas

Una vez configurado, puedes usar las herramientas directamente en el chat de tu editor.

#### Herramientas Disponibles:

**1. `learn_text(text, source_name)` - A√±adir informaci√≥n textual**
```
@rag_server_knowledge learn_text("El punto de fusi√≥n del titanio es 1,668 ¬∞C.", "material_properties")
```
- **Cu√°ndo usar**: Para a√±adir hechos, definiciones, notas de conversaci√≥n, etc.
- **Par√°metros**: 
  - `text`: El contenido a almacenar
  - `source_name`: Nombre descriptivo de la fuente (opcional, por defecto "manual_input")

**2. `learn_document(file_path)` - Procesar documentos**
```
@rag_server_knowledge learn_document("C:\\Reportes\\informe_q3.pdf")
```
- **Cu√°ndo usar**: Para procesar archivos PDF, DOCX, PPTX, XLSX, TXT, HTML, CSV, JSON, XML
- **Caracter√≠sticas**: 
  - Conversi√≥n autom√°tica a Markdown
  - Copia guardada en `./converted_docs/`
  - Metadatos de fuente autom√°ticos

**3. `ask_rag(query)` - Consultar informaci√≥n**
```
@rag_server_knowledge ask_rag("¬øCu√°l es el punto de fusi√≥n del titanio?")
```
- **Cu√°ndo usar**: Para buscar informaci√≥n previamente almacenada
- **Respuesta incluye**: 
  - Respuesta generada por IA
  - üìö Lista de fuentes utilizadas

#### Ejemplo de Flujo Completo:

```bash
# 1. A√±adir informaci√≥n
@rag_server_knowledge learn_text("La temperatura de fusi√≥n del titanio es 1,668¬∞C.", "material_properties")

# 2. Procesar un documento
@rag_server_knowledge learn_document("C:\\Documents\\manual_titanio.pdf")

# 3. Hacer preguntas
@rag_server_knowledge ask_rag("¬øCu√°l es la temperatura de fusi√≥n del titanio?")
```

**Respuesta esperada:**
```
La temperatura de fusi√≥n del titanio es 1,668¬∞C.

üìö Fuentes de informaci√≥n:
   1. material_properties
   2. manual_titanio.pdf
```

---

## üß™ Pruebas y Verificaci√≥n

### Probar el Sistema

Para verificar que todo funciona correctamente:

```bash
# Probar el sistema RAG con metadatos de fuente
python test_rag.py
```

Este script realizar√° pruebas autom√°ticas y mostrar√° las fuentes de informaci√≥n utilizadas.

### Verificar la Base de Datos

Los documentos procesados se almacenan en:
- **Base de datos vectorial**: `./rag_mcp_db/`
- **Copias Markdown**: `./converted_docs/`

---

## ü§ñ Uso por Agentes de IA

El sistema est√° optimizado para ser utilizado por agentes de IA. Consulta `AGENT_INSTRUCTIONS.md` para:

- Gu√≠as detalladas de uso
- Ejemplos de casos de uso
- Mejores pr√°cticas
- Manejo de errores
- Consideraciones importantes

### Caracter√≠sticas para Agentes:

- **Descripciones detalladas** de cada herramienta
- **Ejemplos de uso** claros y espec√≠ficos
- **Manejo de errores inteligente** con sugerencias √∫tiles
- **Metadatos de fuente** para rastreabilidad completa
- **Respuestas estructuradas** con informaci√≥n de fuentes

---

## üìÇ Estructura del Proyecto

```
/
‚îú‚îÄ‚îÄ .venv/                  # Entorno virtual de Python
‚îú‚îÄ‚îÄ rag_mcp_db/             # Base de datos vectorial (se crea al usarla)
‚îú‚îÄ‚îÄ converted_docs/          # Copias en Markdown de documentos procesados
‚îú‚îÄ‚îÄ bulk_ingest.py          # Script para la ingesta masiva de documentos
‚îú‚îÄ‚îÄ rag_core.py             # L√≥gica central y reutilizable del sistema RAG
‚îú‚îÄ‚îÄ rag_server.py           # El servidor MCP (lanzado por run_server.bat)
‚îú‚îÄ‚îÄ run_server.bat          # Script de arranque para el servidor en Windows
‚îú‚îÄ‚îÄ requirements.txt        # Dependencias completas (recomendado)
‚îú‚îÄ‚îÄ requirements-minimal.txt # Dependencias m√≠nimas para pruebas r√°pidas
‚îú‚îÄ‚îÄ requirements-dev.txt    # Dependencias de desarrollo
‚îú‚îÄ‚îÄ pre_download_model.py   # Script para pre-descargar el modelo de embedding
‚îú‚îÄ‚îÄ test_rag.py             # Script de prueba del sistema RAG
‚îú‚îÄ‚îÄ AGENT_INSTRUCTIONS.md   # Gu√≠a para agentes de IA
‚îú‚îÄ‚îÄ proyecto_alpha.txt      # Archivo de ejemplo
‚îî‚îÄ‚îÄ README.md               # Este archivo
```