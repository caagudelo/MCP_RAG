# Servidor RAG Personal con MCP

Este proyecto implementa un servidor compatible con el Protocolo de Contexto de Modelo (MCP) que dota a los clientes de IA (como Cursor, Claude for Desktop, etc.) de una capacidad de Recuperaci√≥n Aumentada por Generaci√≥n (RAG). Permite al modelo de lenguaje acceder a una base de conocimiento privada y local, alimentada por tus propios textos y documentos.

## ‚ú® Caracter√≠sticas

- **Memoria Persistente para tu IA:** "Ense√±a" a tu IA nueva informaci√≥n que recordar√° entre sesiones.
- **üÜï Interfaz Gr√°fica de Usuario (GUI):** Una aplicaci√≥n de escritorio intuitiva (`run_gui.bat`) para procesar documentos, previsualizarlos y seleccionarlos antes de a√±adirlos a la base de conocimiento.
- **Procesamiento de Documentos:** Alimenta la base de conocimiento con archivos `.pdf`, `.docx`, `.pptx`, `.txt`, y m√°s.
- **LLM Local y Privado:** Utiliza modelos de lenguaje locales a trav√©s de [Ollama](https://ollama.com/) (ej. Llama 3, Mistral), asegurando que tus datos y preguntas nunca salgan de tu m√°quina.
- **100% Local y Offline:** Tanto el modelo de lenguaje como los embeddings se ejecutan en tu m√°quina. Ning√∫n dato sale a internet. Una vez descargados los modelos, funciona sin conexi√≥n.
- **Ingesta Masiva:** Un script dedicado para procesar directorios enteros de documentos y construir la base de conocimiento de manera eficiente.
- **Arquitectura Modular:** La l√≥gica del RAG est√° separada de los scripts de servidor y de ingesta, facilitando el mantenimiento y la expansi√≥n.
- **Copias en Markdown:** Cada documento procesado se guarda autom√°ticamente en formato Markdown para verificaci√≥n y reutilizaci√≥n.
- **üÜï Metadatos de Fuente:** Rastreabilidad completa de informaci√≥n con atribuci√≥n de fuentes en cada respuesta.
- **üÜï Optimizado para Agentes de IA:** Descripciones detalladas y manejo de errores inteligente para uso efectivo por agentes de IA.

---

## üèóÔ∏è Arquitectura

El proyecto est√° dividido en tres componentes principales:

1.  `rag_core.py`: El coraz√≥n del sistema. Contiene toda la l√≥gica reutilizable para manejar la base de datos vectorial (ChromaDB), procesar texto y crear la cadena de preguntas y respuestas con LangChain. **Incluye soporte para metadatos de fuente.**
2.  `rag_server.py`: El servidor MCP. Expone las herramientas (`learn_text`, `learn_document`, `ask_rag`) que el cliente de IA puede invocar. Se comunica a trav√©s de `stdio`. **Optimizado con descripciones detalladas para agentes de IA.**
3.  `bulk_ingest.py`: Un script de l√≠nea de comandos para procesar una carpeta llena de documentos y a√±adirlos a la base de conocimiento de forma masiva. **Incluye metadatos de fuente autom√°ticos.**

### Archivos de Documentaci√≥n:
- `AGENT_INSTRUCTIONS.md`: Gu√≠a completa para agentes de IA sobre c√≥mo usar el sistema
- `test_rag.py`: Script de prueba para verificar el funcionamiento del sistema

---

## üöÄ Gu√≠a de Instalaci√≥n y Configuraci√≥n

Sigue estos pasos para poner en marcha el sistema.

### Prerrequisitos

- **Python 3.10+**
- **Ollama:** Aseg√∫rate de que [Ollama est√© instalado](https://ollama.com/) y en ejecuci√≥n en tu sistema.

### 1. Instalaci√≥n (¬°Autom√°tica!)

Gracias a los nuevos scripts de arranque, la instalaci√≥n es incre√≠blemente sencilla.

1.  **Para el Servidor RAG:** Simplemente ejecuta `run_server.bat`.
2.  **Para la Ingesta de Documentos:** Simplemente ejecuta `run_gui.bat`.

La primera vez que ejecutes cualquiera de estos archivos, el script har√° todo por ti:
- ‚úÖ Crear√° un entorno virtual de Python en una carpeta `.venv`.
- ‚úÖ Activar√° el entorno.
- ‚úÖ Instalar√° todas las dependencias necesarias desde `requirements.txt`.
- ‚úÖ Lanzar√° la aplicaci√≥n.

En ejecuciones posteriores, el script simplemente activar√° el entorno y lanzar√° la aplicaci√≥n directamente.

### 2. Configuraci√≥n de Ollama (Paso Cr√≠tico)

Ollama es necesario para que el sistema RAG funcione, ya que proporciona el modelo de lenguaje local que genera las respuestas.

#### Instalaci√≥n de Ollama

**Windows:**
1. Descarga Ollama desde [ollama.com](https://ollama.com/)
2. Ejecuta el instalador y sigue las instrucciones
3. Ollama se ejecutar√° autom√°ticamente como servicio

**macOS/Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

#### Verificar Instalaci√≥n

```bash
# Verificar que Ollama est√° funcionando
ollama --version

# Verificar que el servicio est√° ejecut√°ndose
ollama list
```

#### Descargar Modelos de Lenguaje

El sistema RAG necesita un modelo de lenguaje para generar respuestas. Recomendamos:

```bash
# Modelo recomendado (equilibrio entre velocidad y calidad)
ollama pull llama3

# Alternativas m√°s r√°pidas
ollama pull phi3
ollama pull mistral

# Alternativa m√°s potente (requiere m√°s recursos)
ollama pull llama3.1:8b
```

#### Configurar el Modelo en el Sistema

Una vez descargado el modelo, aseg√∫rate de que `rag_core.py` use el modelo correcto:

```python
# En rag_core.py, l√≠nea ~100, verifica que use tu modelo:
llm = ChatOllama(model="llama3", temperature=0)
```

**Nota:** Si descargaste un modelo diferente, cambia `"llama3"` por el nombre de tu modelo.

#### Probar Ollama

```bash
# Probar que el modelo funciona
ollama run llama3 "Hola, ¬øc√≥mo est√°s?"
```

Si ves una respuesta generada, Ollama est√° funcionando correctamente.

#### Soluci√≥n de Problemas Comunes

**Error: "Ollama is not running"**
```bash
# Iniciar Ollama manualmente
ollama serve
```

**Error: "Model not found"**
```bash
# Verificar modelos disponibles
ollama list

# Descargar el modelo si no est√°
ollama pull llama3
```

**Error: "Out of memory"**
- Usa un modelo m√°s peque√±o: `ollama pull phi3`
- Cierra otras aplicaciones que consuman mucha RAM
- Considera aumentar la memoria virtual en Windows

### 2. Verificaci√≥n Completa del Sistema

Antes de continuar, vamos a verificar que todo est√© funcionando correctamente:

#### Paso 1: Verificar Ollama
```bash
# Verificar que Ollama est√° ejecut√°ndose
ollama list

# Probar el modelo
ollama run llama3 "Test de funcionamiento"
```

#### Paso 2: Verificar Dependencias de Python
```bash
# Verificar que todas las dependencias est√°n instaladas
python -c "import mcp; print('‚úÖ MCP instalado correctamente')"
python -c "import langchain; print('‚úÖ LangChain instalado correctamente')"
python -c "import chromadb; print('‚úÖ ChromaDB instalado correctamente')"
```

#### Paso 3: Probar el Sistema RAG
```bash
# Ejecutar el script de prueba
python test_rag.py
```

Si todo funciona correctamente, ver√°s:
- ‚úÖ Ollama respondiendo a comandos
- ‚úÖ Todas las dependencias import√°ndose sin errores
- ‚úÖ El sistema RAG procesando preguntas y mostrando fuentes

**¬°Tu sistema RAG est√° listo para usar!** üöÄ

---

## üõ†Ô∏è Gu√≠a de Uso

### Uso 1: Poblar la Base de Conocimiento con la GUI (Recomendado)

La forma m√°s f√°cil e intuitiva de a√±adir documentos es usando la interfaz gr√°fica.

1.  Haz doble clic en `run_gui.bat`.
2.  La aplicaci√≥n se iniciar√° (la primera vez puede tardar mientras instala las dependencias).
3.  Usa el bot√≥n "Explorar..." para seleccionar la carpeta con tus documentos.
4.  Haz clic en "Iniciar Procesamiento". Los archivos se convertir√°n a Markdown en memoria.
5.  Ve a la pesta√±a "Revisi√≥n", selecciona los archivos que quieres guardar y previsualiza su contenido.
6.  Ve a la pesta√±a "Almacenamiento" y haz clic en "Iniciar Almacenamiento" para guardar los documentos seleccionados en la base de datos.

### Uso 2: Poblar la Base de Conocimiento desde la L√≠nea de Comandos

Si prefieres usar la l√≠nea de comandos o necesitas automatizar la ingesta.

1.  Abre una terminal.
2.  Activa el entorno virtual: `.\.venv\Scripts\activate`.
3.  Ejecuta el script `bulk_ingest.py` apuntando a tu carpeta de documentos:
    ```bash
    python bulk_ingest.py --directory "C:\Ruta\A\Tus\Documentos"
    ```

### Uso 3: Configuraci√≥n del Cliente MCP (Ej. Cursor)

Para que tu editor de IA pueda usar el servidor, debes configurarlo.

1.  **Encuentra el archivo de configuraci√≥n de servidores MCP de tu editor.** Para Cursor, busca un archivo como `mcp_servers.json` en su directorio de configuraci√≥n (`%APPDATA%\cursor` en Windows). Si no existe, puedes crearlo.

2.  **A√±ade la siguiente configuraci√≥n al archivo JSON.**
    
    Este m√©todo utiliza un script de arranque (`run_server.bat`) para asegurar que la codificaci√≥n de caracteres sea UTF-8, previniendo errores en Windows.

    **¬°IMPORTANTE!** Debes reemplazar `"D:\\ruta\\completa\\a\\tu\\proyecto\\MCP_RAG"` con la ruta absoluta real a la carpeta de este proyecto en tu m√°quina.

    ```json
    {
      "mcpServers": {
        "rag_server_knowledge": {
          "command": "D:\\ruta\\completa\\a\\tu\\proyecto\\MCP_RAG\\run_server.bat",
          "args": [],
          "workingDirectory": "D:\\ruta\\completa\\a\\tu\\proyecto\\MCP_RAG"
        }
      }
    }
    ```

3.  **Reinicia tu editor.** Al arrancar, deber√≠a detectar y lanzar tu `run_server.bat`, que a su vez ejecutar√° `rag_server.py` en segundo plano con el entorno correcto.

### Uso 4: Interactuando con las Herramientas

Una vez configurado, puedes usar las herramientas directamente en el chat de tu editor.

#### Herramientas Disponibles:

**1. `learn_text(text, source_name)` - A√±adir informaci√≥n textual**
```
@rag_server_knowledge learn_text("El punto de fusi√≥n del titanio es 1,668 ¬∞C.", "material_properties")
```
- **Cu√°ndo usar**: Para a√±adir hechos, definiciones, notas de conversaci√≥n, etc.
- **Par√°metros**: 
  - `text`: El contenido a almacenar
  - `source_name`: Nombre descriptivo de la fuente (opcional, por defecto "manual_input")

**2. `learn_document(file_path)` - Procesar documentos**
```
@rag_server_knowledge learn_document("C:\\Reportes\\informe_q3.pdf")
```
- **Cu√°ndo usar**: Para procesar archivos PDF, DOCX, PPTX, XLSX, TXT, HTML, CSV, JSON, XML
- **Caracter√≠sticas**: 
  - Conversi√≥n autom√°tica a Markdown
  - Copia guardada en `./converted_docs/`
  - Metadatos de fuente autom√°ticos

**3. `ask_rag(query)` - Consultar informaci√≥n**
```
@rag_server_knowledge ask_rag("¬øCu√°l es el punto de fusi√≥n del titanio?")
```
- **Cu√°ndo usar**: Para buscar informaci√≥n previamente almacenada
- **Respuesta incluye**: 
  - Respuesta generada por IA
  - üìö Lista de fuentes utilizadas

#### Ejemplo de Flujo Completo:

```bash
# 1. A√±adir informaci√≥n
@rag_server_knowledge learn_text("La temperatura de fusi√≥n del titanio es 1,668¬∞C.", "material_properties")

# 2. Procesar un documento
@rag_server_knowledge learn_document("C:\\Documents\\manual_titanio.pdf")

# 3. Hacer preguntas
@rag_server_knowledge ask_rag("¬øCu√°l es la temperatura de fusi√≥n del titanio?")
```

**Respuesta esperada:**
```
La temperatura de fusi√≥n del titanio es 1,668¬∞C.

üìö Fuentes de informaci√≥n:
   1. material_properties
   2. manual_titanio.pdf
```

---

## üß™ Pruebas y Verificaci√≥n

### Probar el Sistema

Para verificar que todo funciona correctamente:

```bash
# Probar el sistema RAG con metadatos de fuente
python test_rag.py
```

Este script realizar√° pruebas autom√°ticas y mostrar√° las fuentes de informaci√≥n utilizadas.

### Verificar la Base de Datos

Los documentos procesados se almacenan en:
- **Base de datos vectorial**: `./rag_mcp_db/`
- **Copias Markdown**: `./converted_docs/`

---

## ü§ñ Uso por Agentes de IA

El sistema est√° optimizado para ser utilizado por agentes de IA. Consulta `AGENT_INSTRUCTIONS.md` para:

- Gu√≠as detalladas de uso
- Ejemplos de casos de uso
- Mejores pr√°cticas
- Manejo de errores
- Consideraciones importantes

### Caracter√≠sticas para Agentes:

- **Descripciones detalladas** de cada herramienta
- **Ejemplos de uso** claros y espec√≠ficos
- **Manejo de errores inteligente** con sugerencias √∫tiles
- **Metadatos de fuente** para rastreabilidad completa
- **Respuestas estructuradas** con informaci√≥n de fuentes

---

## üîß Optimizaciones Implementadas

Esta secci√≥n explica c√≥mo funciona el sistema RAG optimizado actualmente, con todas las mejoras t√©cnicas implementadas para obtener las mejores b√∫squedas y respuestas.

### **A. Divisi√≥n Inteligente de Texto**

#### **¬øC√≥mo funciona la divisi√≥n de texto?**

El sistema utiliza `RecursiveCharacterTextSplitter` que divide el texto de manera inteligente, respetando la estructura natural del contenido:

```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # Tama√±o m√°ximo de cada fragmento
    chunk_overlap=200,      # Caracteres que se comparten entre fragmentos
    length_function=len,    # Funci√≥n para medir longitud
    separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""]  # Separadores inteligentes
)
```

#### **Separadores Inteligentes:**

El sistema busca los mejores puntos de divisi√≥n en este orden:
1. **`\n\n`** - P√°rrafos (mejor opci√≥n)
2. **`\n`** - Saltos de l√≠nea
3. **`. `** - Final de oraciones
4. **`! `** - Final de exclamaciones
5. **`? `** - Final de preguntas
6. **` `** - Espacios (√∫ltimo recurso)

#### **¬øPor qu√© es importante?**

- **Preserva Contexto**: No corta en medio de una idea
- **Mantiene Coherencia**: Cada fragmento es una unidad l√≥gica
- **Mejora B√∫squedas**: Los fragmentos son m√°s relevantes y completos

#### **Ejemplo de Divisi√≥n Inteligente:**
```python
# Texto original:
"""
La inteligencia artificial (IA) es una rama de la inform√°tica. 
Se enfoca en crear sistemas inteligentes. Estos sistemas pueden 
aprender y tomar decisiones. La IA tiene muchas aplicaciones 
en la vida moderna.
"""

# Fragmentos resultantes:
# Fragmento 1: "La inteligencia artificial (IA) es una rama de la inform√°tica. Se enfoca en crear sistemas inteligentes."
# Fragmento 2: "Estos sistemas pueden aprender y tomar decisiones. La IA tiene muchas aplicaciones en la vida moderna."
```

### **B. Motor de B√∫squeda Optimizado**

#### **Configuraci√≥n Actual:**

```python
retriever = vector_store.as_retriever(
    search_type="similarity",  # B√∫squeda por similitud sem√°ntica
    search_kwargs={
        "k": 5,                # Recupera 5 fragmentos m√°s relevantes
        "score_threshold": 0.7, # Solo documentos con similitud > 70%
        "fetch_k": 10          # Busca 10 documentos y filtra los mejores 5
    }
)
```

#### **¬øC√≥mo funciona la b√∫squeda?**

1. **B√∫squeda Inicial**: Busca 10 documentos candidatos
2. **C√°lculo de Similitud**: Calcula qu√© tan similares son a tu pregunta
3. **Filtrado por Calidad**: Solo mantiene documentos con similitud > 70%
4. **Selecci√≥n Final**: Toma los 5 mejores fragmentos

#### **Par√°metros Explicados:**

- **`k=5`**: Obtienes informaci√≥n de 5 fuentes diferentes para respuestas m√°s completas
- **`score_threshold=0.7`**: Garantiza que solo se use informaci√≥n muy relevante
- **`fetch_k=10`**: Busca m√°s opciones para seleccionar las mejores

### **C. Limpieza Autom√°tica de Texto**

#### **¬øQu√© hace la limpieza?**

Antes de procesar cualquier texto, el sistema lo limpia autom√°ticamente:

```python
def clean_text_for_rag(text: str) -> str:
    # Eliminar espacios m√∫ltiples
    text = re.sub(r'\s+', ' ', text)
    
    # Mantener solo caracteres importantes
    text = re.sub(r'[^\w\s\.\,\!\?\;\:\-\(\)\[\]\{\}\"\']', '', text)
    
    # Normalizar puntuaci√≥n
    text = re.sub(r'\s+([\.\,\!\?\;\:])', r'\1', text)
    
    return text.strip()
```

#### **Problemas que resuelve autom√°ticamente:**

1. **Espacios M√∫ltiples**: `"Hola    mundo"` ‚Üí `"Hola mundo"`
2. **Caracteres Especiales**: `"Texto@#$%^"` ‚Üí `"Texto"`
3. **Puntuaci√≥n Inconsistente**: `"Hola . Mundo"` ‚Üí `"Hola. Mundo"`
4. **Saltos de L√≠nea Excesivos**: Normaliza el formato

#### **Ejemplo de Limpieza:**
```python
# Texto con ruido:
"""
La IA    es muy importante!!!
Tiene muchas aplicaciones@@@
"""

# Despu√©s de limpieza autom√°tica:
"La IA es muy importante! Tiene muchas aplicaciones"
```

### **D. Respuestas Enriquecidas con Informaci√≥n de Calidad**

#### **¬øQu√© informaci√≥n incluye cada respuesta?**

El sistema proporciona respuestas completas con:

```
ü§ñ Respuesta:
[Respuesta generada por IA]

üìö Fuentes de informaci√≥n utilizadas:
   1. documento1.pdf (.pdf) - Procesado: 15/12/2024 14:30
   2. manual_ia.txt (.txt) - Procesado: 15/12/2024 14:25

‚úÖ Alta confianza: Respuesta basada en m√∫ltiples fuentes
```

#### **Informaci√≥n Incluida:**

1. **Respuesta Principal**: Generada por el modelo de IA
2. **Fuentes Utilizadas**: Lista de documentos consultados
3. **Tipo de Archivo**: Formato de cada fuente
4. **Fecha de Procesamiento**: Cu√°ndo se a√±adi√≥ a la base de datos
5. **Nivel de Confianza**: Basado en el n√∫mero de fuentes

#### **Niveles de Confianza:**

- **‚úÖ Alta confianza**: 3 o m√°s fuentes
- **‚ö†Ô∏è Confianza media**: 2 fuentes
- **‚ö†Ô∏è Confianza limitada**: 1 fuente

### **E. Sistema de Logs en Espa√±ol**

#### **¬øQu√© informaci√≥n muestran los logs?**

Los logs te permiten seguir todo el proceso en espa√±ol:

```
MCP Server: Iniciando servidor MCP RAG...
MCP Server: Calentando sistema RAG...
MCP Server: Precargando modelo de embedding en memoria...
Core: Cargando modelo de embedding local: all-MiniLM-L6-v2
Core: Este paso puede tomar unos minutos en la primera ejecuci√≥n para descargar el modelo.
Core: Usando dispositivo 'cpu' para embeddings.
Core: ¬°Modelo cargado exitosamente!
Core: Inicializando base de datos vectorial...
Core: Base de datos vectorial inicializada en './rag_mcp_db'
MCP Server: Sistema RAG caliente y listo.
```

#### **Informaci√≥n que puedes monitorear:**

- **Progreso de Carga**: Cu√°ndo se cargan los modelos
- **Procesamiento de Texto**: Cu√°ntos fragmentos se crean
- **B√∫squedas**: Cu√°ntas fuentes se encuentran
- **Errores**: Mensajes claros con sugerencias

### **F. Manejo Inteligente de Errores**

#### **¬øC√≥mo responde el sistema a los errores?**

Cuando algo no funciona correctamente, el sistema proporciona:

```
‚ùå Error al procesar la pregunta: [Descripci√≥n del error]

üí° Sugerencias:
- Verifica que el sistema RAG est√© correctamente inicializado
- Intenta reformular tu pregunta
- Si el problema persiste, reinicia el servidor
```

#### **Tipos de errores que maneja:**

- **Archivos no encontrados**: Sugiere verificar rutas
- **Formatos no soportados**: Lista formatos compatibles
- **Problemas de permisos**: Gu√≠a para verificar acceso
- **Sistema no inicializado**: Instrucciones de reinicio

## **¬øC√≥mo Funciona el Sistema Optimizado?**

### **1. Proceso de B√∫squeda Completo:**

1. **Recepci√≥n de Pregunta**: El sistema recibe tu consulta
2. **Limpieza Autom√°tica**: Limpia la pregunta si es necesario
3. **B√∫squeda Sem√°ntica**: Encuentra documentos relevantes
4. **Filtrado por Calidad**: Solo usa informaci√≥n muy similar
5. **Generaci√≥n de Respuesta**: Crea respuesta basada en m√∫ltiples fuentes
6. **Informaci√≥n de Fuentes**: Proporciona lista completa de referencias

### **2. Caracter√≠sticas de Calidad:**

- **Alta Precisi√≥n**: Solo documentos con >70% de similitud
- **Contexto Completo**: 5 fragmentos de informaci√≥n
- **Trazabilidad**: Sabes exactamente de d√≥nde viene cada informaci√≥n
- **Confianza Medible**: Nivel de confianza basado en fuentes

### **3. Experiencia de Usuario:**

- **Respuestas Completas**: Informaci√≥n detallada y bien estructurada
- **Fuentes Claras**: Sabes qu√© documentos se consultaron
- **Errores √ötiles**: Mensajes claros con sugerencias
- **Monitoreo F√°cil**: Logs en espa√±ol para seguir el proceso

## **Ejemplo de Funcionamiento Completo**

**Pregunta**: "¬øCu√°les son las aplicaciones de machine learning en medicina?"

**Proceso Interno:**
1. Sistema busca documentos sobre "machine learning" y "medicina"
2. Encuentra 3 documentos relevantes con similitud >70%
3. Genera respuesta combinando informaci√≥n de las 3 fuentes
4. Proporciona lista completa de fuentes utilizadas

**Respuesta Final:**
```
ü§ñ Respuesta:
Machine learning tiene m√∫ltiples aplicaciones en medicina, incluyendo diagn√≥stico por im√°genes, an√°lisis de datos m√©dicos, descubrimiento de f√°rmacos y medicina personalizada. Los algoritmos pueden analizar radiograf√≠as, resonancias magn√©ticas y otros estudios m√©dicos para detectar enfermedades con alta precisi√≥n.

üìö Fuentes de informaci√≥n utilizadas:
   1. aplicaciones_ml.pdf (.pdf) - Procesado: 15/12/2024 14:30
   2. medicina_digital.txt (.txt) - Procesado: 15/12/2024 14:25
   3. ia_salud.docx (.docx) - Procesado: 15/12/2024 14:20

‚úÖ Alta confianza: Respuesta basada en m√∫ltiples fuentes
```

## **Consejos para Obtener Mejores Resultados**

### **1. A√±ade Informaci√≥n Variada:**
```python
# Ejemplo de uso
learn_text("La inteligencia artificial es una rama de la inform√°tica que busca crear sistemas capaces de realizar tareas que requieren inteligencia humana.", "definicion_ia")
```

### **2. Usa Preguntas Espec√≠ficas:**
- ‚ùå "¬øQu√© es la IA?"
- ‚úÖ "¬øCu√°les son las principales aplicaciones de la inteligencia artificial en la medicina?"

### **3. Verifica las Fuentes:**
- Siempre revisa la informaci√≥n de fuentes en las respuestas
- Usa m√∫ltiples documentos sobre el mismo tema para mayor confianza

### **4. Monitoreo del Sistema:**
- Los logs te mostrar√°n cu√°ntos fragmentos se procesan
- Ver√°s informaci√≥n sobre la calidad de las b√∫squedas
- Podr√°s identificar si necesitas ajustar par√°metros

---

## üß† Entendiendo los Embeddings

Esta secci√≥n explica qu√© son los embeddings y por qu√© son fundamentales para el funcionamiento del sistema RAG.

### **ü§ñ ¬øQu√© son los Embeddings?**

#### **Definici√≥n Simple:**
Los **embeddings** son como "traductores" que convierten texto en n√∫meros que las computadoras pueden entender y comparar. Es como crear un "c√≥digo postal" para cada palabra o frase.

#### **Analog√≠a Pr√°ctica:**
Imagina que tienes una biblioteca con miles de libros. Para encontrar libros similares, podr√≠as:
- **Sin embeddings**: Leer cada libro completo (muy lento)
- **Con embeddings**: Usar un c√≥digo que describe el contenido (muy r√°pido)

### **üî¢ ¬øC√≥mo Funcionan los Embeddings?**

#### **Proceso de Conversi√≥n:**
```python
# Texto original (humano entiende)
"La inteligencia artificial es fascinante"

# Embedding (computadora entiende)
[0.234, -0.567, 0.891, 0.123, -0.456, ...]  # Vector de 384 n√∫meros
```

#### **¬øPor qu√© N√∫meros?**
- **Comparaci√≥n r√°pida**: Las computadoras pueden comparar n√∫meros muy r√°pido
- **Similitud matem√°tica**: Textos similares tienen n√∫meros similares
- **B√∫squeda eficiente**: Encuentra informaci√≥n relevante en milisegundos

### **üéØ ¬øC√≥mo Se Usan en tu Sistema RAG?**

#### **1. Proceso de Almacenamiento:**
```python
# Cuando a√±ades texto al sistema:
texto = "Machine learning es un tipo de IA"
embedding = modelo_embedding.convertir_a_vector(texto)
# Resultado: [0.1, 0.5, -0.3, 0.8, ...]

# Se guarda en la base de datos vectorial
base_datos.guardar(texto, embedding)
```

#### **2. Proceso de B√∫squeda:**
```python
# Cuando haces una pregunta:
pregunta = "¬øQu√© es machine learning?"
embedding_pregunta = modelo_embedding.convertir_a_vector(pregunta)
# Resultado: [0.12, 0.48, -0.25, 0.82, ...]

# El sistema busca textos con embeddings similares
resultados = base_datos.buscar_similares(embedding_pregunta)
```

### **üßÆ ¬øC√≥mo Se Calcula la Similitud?**

#### **C√°lculo de Distancia:**
```python
# Ejemplo simplificado:
embedding_1 = [0.1, 0.5, -0.3, 0.8]
embedding_2 = [0.12, 0.48, -0.25, 0.82]

# Distancia = qu√© tan diferentes son
distancia = calcular_distancia(embedding_1, embedding_2)
# Resultado: 0.05 (muy similar)

# Similitud = 1 - distancia
similitud = 1 - 0.05 = 0.95 (95% similar)
```

#### **Interpretaci√≥n de Similitud:**
- **0.9 - 1.0**: Muy similar (excelente coincidencia)
- **0.7 - 0.9**: Similar (buena coincidencia) ‚Üê **Tu sistema usa 0.7 como m√≠nimo**
- **0.5 - 0.7**: Moderadamente similar
- **0.0 - 0.5**: Poco similar

### **üîß ¬øQu√© Modelo de Embedding Usa tu Sistema?**

#### **Modelo Actual:**
```python
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"
```

#### **Caracter√≠sticas del Modelo:**
- **Tama√±o**: ~90MB (peque√±o y eficiente)
- **Dimensiones**: 384 n√∫meros por texto
- **Idiomas**: Multiling√ºe (espa√±ol e ingl√©s)
- **Velocidad**: Muy r√°pido
- **Calidad**: Excelente para b√∫squedas

#### **¬øPor qu√© Este Modelo?**
- **Eficiente**: No necesita mucha memoria
- **R√°pido**: Procesa texto en milisegundos
- **Preciso**: Encuentra informaci√≥n muy relevante
- **Local**: Funciona sin internet

### **üìä Ejemplo Pr√°ctico en tu Sistema**

#### **Escenario: Buscar informaci√≥n sobre "machine learning"**

**Paso 1: Procesar Documentos**
```python
# Documento 1
texto_1 = "Machine learning es una rama de la IA"
embedding_1 = [0.1, 0.5, -0.3, 0.8, ...]  # 384 n√∫meros

# Documento 2  
texto_2 = "Los algoritmos de ML pueden aprender"
embedding_2 = [0.12, 0.48, -0.25, 0.82, ...]  # 384 n√∫meros

# Documento 3
texto_3 = "El clima hoy est√° soleado"
embedding_3 = [-0.8, 0.2, 0.9, -0.1, ...]  # 384 n√∫meros
```

**Paso 2: Hacer Pregunta**
```python
pregunta = "¬øQu√© es machine learning?"
embedding_pregunta = [0.11, 0.49, -0.28, 0.81, ...]
```

**Paso 3: Calcular Similitudes**
```python
similitud_1 = calcular_similitud(embedding_pregunta, embedding_1)  # 0.95
similitud_2 = calcular_similitud(embedding_pregunta, embedding_2)  # 0.92
similitud_3 = calcular_similitud(embedding_pregunta, embedding_3)  # 0.15
```

**Paso 4: Seleccionar Resultados**
```python
# Solo documentos con similitud > 0.7 (70%)
resultados = [
    (texto_1, 0.95),  # Muy relevante
    (texto_2, 0.92)   # Muy relevante
    # texto_3 se descarta (0.15 < 0.7)
]
```

### **‚ö° Ventajas de los Embeddings**

#### **1. B√∫squeda Sem√°ntica:**
```python
# Encuentra informaci√≥n incluso con palabras diferentes
pregunta = "¬øQu√© es ML?"
# Encuentra: "Machine learning es una rama de la IA"
# Aunque "ML" y "Machine learning" son diferentes
```

#### **2. Velocidad:**
- **Sin embeddings**: Leer todos los documentos (muy lento)
- **Con embeddings**: Comparar n√∫meros (muy r√°pido)

#### **3. Precisi√≥n:**
- **B√∫squeda por palabras**: "IA" no encuentra "inteligencia artificial"
- **B√∫squeda sem√°ntica**: "IA" encuentra "inteligencia artificial"

#### **4. Escalabilidad:**
- **Miles de documentos**: Procesamiento en segundos
- **Millones de documentos**: Procesamiento en minutos

### **üîç ¬øC√≥mo Se Configuran en tu Sistema?**

#### **En `rag_core.py`:**
```python
def get_embedding_function():
    embeddings = HuggingFaceEmbeddings(
        model_name="all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'}  # o 'cuda' si tienes GPU
    )
    return embeddings
```

#### **Par√°metros Importantes:**
- **`model_name`**: Qu√© modelo usar
- **`device`**: CPU o GPU
- **`chunk_size`**: Tama√±o de fragmentos (1000 caracteres)
- **`chunk_overlap`**: Superposici√≥n entre fragmentos (200 caracteres)

### **üìà ¬øC√≥mo Mejorar los Embeddings?**

#### **1. Calidad del Texto:**
```python
# ‚úÖ Texto limpio y bien estructurado
"Machine learning es una rama de la inteligencia artificial que permite a las computadoras aprender sin ser programadas expl√≠citamente."

# ‚ùå Texto con ruido
"ML is AI stuff that makes computers learn stuff without programming them explicitly."
```

#### **2. Tama√±o de Fragmentos:**
- **Muy peque√±os**: Pierden contexto
- **Muy grandes**: Menos precisos
- **√ìptimo**: 1000 caracteres con 200 de overlap

#### **3. Modelo de Embedding:**
- **Modelos m√°s grandes**: Mejor calidad, m√°s lento
- **Modelos m√°s peque√±os**: M√°s r√°pido, calidad aceptable
- **Tu modelo**: Balance perfecto

### **üéØ Resumen: ¬øPor qu√© son Importantes?**

#### **Sin Embeddings:**
- B√∫squedas lentas
- Resultados imprecisos
- No entiende sin√≥nimos
- Escalabilidad limitada

#### **Con Embeddings:**
- B√∫squedas instant√°neas
- Resultados muy precisos
- Entiende significado
- Escalable a millones de documentos

**Los embeddings son el "cerebro" que hace que tu sistema RAG sea inteligente y r√°pido. Convierten el texto en un lenguaje que las computadoras pueden entender y comparar eficientemente, permitiendo b√∫squedas sem√°nticas precisas en milisegundos.**

---

## ‚ö†Ô∏è Limitaciones y Elecci√≥n del Modelo de Embedding

Esta secci√≥n detalla por qu√© se eligi√≥ el modelo `all-mpnet-base-v2`, sus ventajas y sus limitaciones en comparaci√≥n con otras alternativas.

### **üéØ ¬øPor qu√© `all-mpnet-base-v2`? Un excelente punto medio**

Este modelo fue seleccionado por ofrecer el mejor **equilibrio entre rendimiento y calidad** para una ejecuci√≥n local.

- **Ventajas:**
    - **Alta Calidad:** Ofrece una comprensi√≥n sem√°ntica significativamente mejor que modelos m√°s peque√±os (como `all-MiniLM-L6-v2`). Es muy bueno capturando matices y relaciones complejas en el texto.
    - **Buen Rendimiento:** Aunque es m√°s grande que los modelos "mini", sigue siendo lo suficientemente r√°pido para ejecutarse en CPUs modernas sin tiempos de espera frustrantes.
    - **Muy Popular:** Es uno de los modelos de `sentence-transformers` m√°s usados y mejor valorados, lo que garantiza un buen soporte y rendimiento probado.

- **Desventajas:**
    - **Uso de Recursos:** Requiere m√°s RAM y espacio en disco (420MB) que los modelos peque√±os.
    - **No es el mejor:** Modelos comerciales de vanguardia (como los de OpenAI o Cohere) o modelos locales mucho m√°s grandes (de varios Gigabytes) pueden ofrecer una precisi√≥n a√∫n mayor, pero a costa de no poder ejecutarse localmente o requerir hardware muy potente.

### **‚öñÔ∏è Comparativa de Modelos**

| Modelo | Tama√±o | Dimensiones | Calidad Sem√°ntica | Requisitos | Ideal para... |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **`all-mpnet-base-v2` (Tu modelo)** | **~420MB** | **768** | **Alta** | **Moderados (CPU/GPU)** | **El mejor balance para uso local y de alta calidad.** |
| `all-MiniLM-L6-v2` | ~90MB | 384 | Media | Bajos (CPU) | Sistemas con muy pocos recursos o donde la velocidad es m√°s importante que la precisi√≥n. |
| `text-embedding-3-large` (OpenAI) | N/A (API) | 3072 | Muy Alta | Conexi√≥n a Internet, API Key | Proyectos comerciales que necesitan la m√°xima precisi√≥n y no tienen problemas de privacidad/coste. |


En resumen, `all-mpnet-base-v2` es la elecci√≥n perfecta para este proyecto: un sistema RAG local, privado y de alto rendimiento que no requiere hardware de servidor.

---

## ‚ö†Ô∏è Limitaciones del Modelo de Embedding

Esta secci√≥n detalla las limitaciones y desventajas del modelo `all-MiniLM-L6-v2` que usa tu sistema, para que puedas tomar decisiones informadas y optimizar su uso.

### **üîç Limitaciones de Tama√±o y Complejidad**

#### **Modelo Peque√±o:**
- **Tama√±o**: Solo 90MB (muy peque√±o)
- **Dimensiones**: 384 n√∫meros (limitado)
- **¬øProblema?** Menos capacidad para capturar matices complejos

#### **Comparaci√≥n con Modelos M√°s Grandes:**
```python
# Tu modelo actual:
all-MiniLM-L6-v2: 90MB, 384 dimensiones

# Modelos m√°s potentes:
sentence-transformers/all-mpnet-base-v2: 420MB, 768 dimensiones
text-embedding-ada-002: 1.5GB, 1536 dimensiones
```

### **üß† Limitaciones en Comprensi√≥n Sem√°ntica**

#### **Contexto Limitado:**
- **Longitud m√°xima**: ~512 tokens por fragmento
- **¬øProblema?** Puede perder contexto en textos largos o complejos

#### **Ejemplo de Limitaci√≥n:**
```python
# Texto complejo que puede ser problem√°tico:
texto_complejo = """
La inteligencia artificial, espec√≠ficamente el machine learning supervisado, 
utiliza algoritmos como redes neuronales convolucionales para procesar 
im√°genes m√©dicas y detectar anomal√≠as en radiograf√≠as de t√≥rax, 
permitiendo diagn√≥sticos m√°s precisos y tempranos.
"""

# El modelo puede no capturar completamente la relaci√≥n entre:
# - "redes neuronales convolucionales" 
# - "procesar im√°genes m√©dicas"
# - "detectar anomal√≠as"
```

### **üåç Limitaciones en Idiomas**

#### **Soporte Multiling√ºe B√°sico:**
- **Idiomas principales**: Ingl√©s y espa√±ol
- **¬øProblema?** Rendimiento desigual en otros idiomas
- **Calidad variable**: Mejor en ingl√©s que en espa√±ol

#### **Ejemplo de Problema:**
```python
# En ingl√©s (excelente):
"machine learning" ‚Üí [0.1, 0.5, -0.3, ...]

# En espa√±ol (bueno, pero no √≥ptimo):
"aprendizaje autom√°tico" ‚Üí [0.08, 0.48, -0.25, ...]

# En otros idiomas (limitado):
"apprentissage automatique" ‚Üí [0.05, 0.45, -0.2, ...]
```

### **üî¨ Limitaciones en Dominios Espec√≠ficos**

#### **Conocimiento General vs Especializado:**
- **Entrenado en**: Texto general de internet
- **¬øProblema?** Puede no entender bien terminolog√≠a t√©cnica espec√≠fica

#### **Ejemplos de Dominios Problem√°ticos:**
```python
# Terminolog√≠a m√©dica especializada:
"adenocarcinoma pulmonar de c√©lulas peque√±as" 
# Puede no capturar bien la relaci√≥n con "c√°ncer de pulm√≥n"

# Terminolog√≠a legal:
"res judicata" 
# Puede no entender que es "cosa juzgada"

# Terminolog√≠a t√©cnica muy espec√≠fica:
"microservicios con arquitectura hexagonal"
# Puede perder matices t√©cnicos espec√≠ficos
```

### **üîó Limitaciones en Comprensi√≥n de Relaciones**

#### **Relaciones Complejas:**
- **Relaciones simples**: Excelente (sin√≥nimos, ant√≥nimos)
- **Relaciones complejas**: Limitado (causalidad, implicaci√≥n)

#### **Ejemplo de Limitaci√≥n:**
```python
# Relaci√≥n simple (funciona bien):
"coche" ‚Üî "autom√≥vil"  # Sin√≥nimos

# Relaci√≥n compleja (puede fallar):
"Si llueve, el suelo se moja" 
# Puede no capturar bien la relaci√≥n causal
```

### **üìù Sensibilidad al Formato del Texto**

#### **Dependencia del Formato:**
- **Texto limpio**: Excelente rendimiento
- **Texto con ruido**: Rendimiento degradado

#### **Ejemplos Problem√°ticos:**
```python
# ‚úÖ Texto limpio (funciona bien):
"La inteligencia artificial es una rama de la inform√°tica."

# ‚ùå Texto con ruido (puede fallar):
"La IA es una rama de la info... muy importante!!!"
"La inteligencia artificial (IA) es una rama de la inform√°tica."
```

### **üéØ Limitaciones en Tareas Espec√≠ficas**

#### **B√∫squeda de Informaci√≥n vs Otras Tareas:**
- **B√∫squeda sem√°ntica**: Excelente
- **Clasificaci√≥n de texto**: Limitado
- **An√°lisis de sentimientos**: No optimizado
- **Extracci√≥n de entidades**: B√°sico

### **üìà Limitaciones de Escalabilidad**

#### **Rendimiento con Grandes Vol√∫menes:**
- **Miles de documentos**: Excelente
- **Millones de documentos**: Puede ser lento
- **¬øPor qu√©?** Comparaci√≥n secuencial de vectores

## **üîÑ Estrategias para Mitigar Limitaciones**

### **1. Optimizar el Texto de Entrada:**
```python
# ‚úÖ Mejorar calidad del texto:
texto_limpio = clean_text_for_rag(texto_original)

# ‚úÖ Usar fragmentos apropiados:
chunk_size = 1000  # Tama√±o √≥ptimo para este modelo
chunk_overlap = 200  # Mantener contexto
```

### **2. Ajustar Par√°metros de B√∫squeda:**
```python
# Para compensar limitaciones:
search_kwargs = {
    "k": 5,                # M√°s fragmentos para mejor cobertura
    "score_threshold": 0.7, # Umbral alto para precisi√≥n
    "fetch_k": 10          # Buscar m√°s candidatos
}
```

### **3. Mejorar la Estructura de Datos:**
```python
# ‚úÖ Documentos bien estructurados:
"Machine learning es una rama de la inteligencia artificial que permite a las computadoras aprender sin ser programadas expl√≠citamente."

# ‚úÖ Metadatos descriptivos:
metadata = {
    "domain": "tecnolog√≠a",
    "language": "espa√±ol",
    "complexity": "intermedio"
}
```

### **4. Considerar Modelos Alternativos (Futuro):**

#### **Para Mejor Calidad:**
```python
# Modelos m√°s potentes (requieren m√°s recursos):
"all-mpnet-base-v2"      # 420MB, mejor calidad
"text-embedding-ada-002" # 1.5GB, m√°xima calidad
```

#### **Para Mejor Velocidad:**
```python
# Modelos m√°s r√°pidos:
"all-MiniLM-L6-v2"       # Tu modelo actual
"paraphrase-MiniLM-L3-v2" # A√∫n m√°s r√°pido
```

## **‚öñÔ∏è Resumen: Ventajas vs Desventajas**

### **Desventajas:**
- ‚ùå Comprensi√≥n sem√°ntica limitada
- ‚ùå Contexto limitado en textos largos
- ‚ùå Rendimiento variable en idiomas
- ‚ùå Limitado en dominios especializados
- ‚ùå Sensible al formato del texto
- ‚ùå Relaciones complejas limitadas

### **Ventajas (que compensan):**
- ‚úÖ Muy r√°pido y eficiente
- ‚úÖ Poco uso de memoria
- ‚úÖ Funciona sin internet
- ‚úÖ Excelente para b√∫squedas b√°sicas
- ‚úÖ Balance calidad/velocidad
- ‚úÖ F√°cil de implementar

## **üéØ Recomendaciones para tu Caso de Uso**

### **Para tu Sistema Actual:**
1. **Mant√©n el modelo actual** - Es un buen balance
2. **Optimiza el texto de entrada** - Limpia y estructura bien
3. **Ajusta par√°metros** - Usa m√°s fragmentos si es necesario
4. **Monitorea resultados** - Verifica calidad de respuestas

### **Para Considerar en el Futuro:**
1. **Si necesitas mejor calidad**: Cambiar a modelo m√°s grande
2. **Si necesitas m√°s velocidad**: Usar modelo m√°s peque√±o
3. **Si tienes GPU**: Habilitar aceleraci√≥n por hardware
4. **Si tienes muchos documentos**: Considerar indexaci√≥n avanzada

### **Se√±ales de que Necesitas un Modelo Mejor:**
- Respuestas inconsistentes en tu dominio
- No encuentra informaci√≥n que sabes que existe
- Problemas con terminolog√≠a t√©cnica espec√≠fica
- Necesitas mayor precisi√≥n en relaciones complejas

---

## ‚ö° Consideraciones para Funcionamiento √ìptimo

Esta secci√≥n detalla las consideraciones t√©cnicas y mejores pr√°cticas para obtener el m√°ximo rendimiento de tu sistema RAG.

### **üîß Requisitos del Sistema**

#### **Memoria RAM:**
- **M√≠nimo recomendado**: 8GB RAM
- **√ìptimo**: 16GB RAM o m√°s
- **¬øPor qu√© es importante?** Los modelos de embedding y el LLM necesitan memoria para funcionar eficientemente

#### **Almacenamiento:**
- **Espacio libre**: Al menos 10GB disponibles
- **Velocidad**: SSD preferiblemente (m√°s r√°pido que HDD)
- **¬øPara qu√©?** Modelos, base de datos vectorial y documentos procesados

#### **CPU/GPU:**
- **CPU**: M√≠nimo 4 n√∫cleos, recomendado 8+ n√∫cleos
- **GPU**: Opcional pero mejora significativamente el rendimiento
- **¬øPor qu√©?** Los embeddings y el procesamiento de texto son intensivos

### **ü§ñ Configuraci√≥n de Ollama**

#### **Modelos Recomendados:**
```bash
# Modelos por rendimiento:
ollama pull llama3        # Equilibrio velocidad/calidad
ollama pull phi3          # M√°s r√°pido, menos recursos
ollama pull mistral       # Buena calidad, moderado uso de recursos
```

#### **Configuraci√≥n de Memoria:**
```bash
# En Windows, ajustar memoria virtual:
# Panel de Control > Sistema > Configuraci√≥n avanzada > Rendimiento > Configuraci√≥n
# Memoria virtual: Al menos 16GB
```

#### **Verificar Funcionamiento:**
```bash
# Probar que Ollama funciona correctamente
ollama list
ollama run llama3 "Test de funcionamiento"
```

### **üìÑ Calidad de los Datos de Entrada**

#### **Documentos Bien Estructurados:**
- **Formato consistente**: Usa el mismo formato en todos los documentos
- **Contenido relevante**: Solo a√±ade informaci√≥n √∫til para tus consultas
- **Tama√±o apropiado**: Documentos entre 1-50 p√°ginas funcionan mejor

#### **Ejemplos de Buena Pr√°ctica:**
```python
# ‚úÖ Documentos bien estructurados
learn_text("La inteligencia artificial es una rama de la inform√°tica que busca crear sistemas capaces de realizar tareas que requieren inteligencia humana. Se divide en machine learning, procesamiento de lenguaje natural y visi√≥n por computadora.", "definicion_ia_completa")

# ‚ùå Informaci√≥n fragmentada
learn_text("IA", "definicion_corta")
learn_text("es", "definicion_fragmentada")
```

### **‚úÇÔ∏è Estrategia de Divisi√≥n de Texto**

#### **Tama√±o de Fragmentos Actual:**
- **Fragmentos**: 1000 caracteres con 200 de overlap
- **Para documentos t√©cnicos**: Puedes aumentar a 1500 caracteres
- **Para conversaciones**: Puedes reducir a 800 caracteres

#### **Separadores Inteligentes:**
El sistema ya usa separadores √≥ptimos, pero puedes ajustar seg√∫n tu contenido:
```python
# Para documentos t√©cnicos con muchas listas:
separators=["\n\n", "\n", ". ", "‚Ä¢ ", "- ", " ", ""]

# Para documentos narrativos:
separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""]
```

### **üîç Configuraci√≥n de B√∫squeda**

#### **Par√°metros Actuales (Optimizados):**
```python
search_kwargs={
    "k": 5,                # 5 fragmentos - buen balance
    "score_threshold": 0.7, # 70% similitud - alta precisi√≥n
    "fetch_k": 10          # 10 candidatos - buena selecci√≥n
}
```

#### **Ajustes seg√∫n Necesidades:**
- **Para respuestas m√°s completas**: Aumentar `k` a 7-8
- **Para mayor precisi√≥n**: Aumentar `score_threshold` a 0.8
- **Para b√∫squedas m√°s amplias**: Reducir `score_threshold` a 0.6

### **üóÑÔ∏è Gesti√≥n de la Base de Datos**

#### **Mantenimiento Regular:**
```bash
# Verificar tama√±o de la base de datos
ls -la rag_mcp_db/

# Limpiar archivos temporales si es necesario
rm -rf rag_mcp_db/*.tmp
```

#### **Backup de Datos:**
```bash
# Crear copia de seguridad
cp -r rag_mcp_db/ rag_mcp_db_backup_$(date +%Y%m%d)
```

### **üí° Optimizaci√≥n de Consultas**

#### **Preguntas Efectivas:**
```python
# ‚úÖ Preguntas espec√≠ficas y claras
ask_rag("¬øCu√°les son las principales aplicaciones de machine learning en el diagn√≥stico m√©dico?")

# ‚ùå Preguntas muy generales
ask_rag("¬øQu√© es la IA?")
```

#### **Uso de Palabras Clave:**
- **Incluye t√©rminos t√©cnicos** espec√≠ficos de tu dominio
- **Usa sin√≥nimos** para conceptos importantes
- **S√© espec√≠fico** en lo que buscas

### **üìä Monitoreo del Rendimiento**

#### **Logs Importantes a Revisar:**
```
Core: Texto dividido en X fragmentos
Core: X fragmentos a√±adidos y guardados en la base de conocimientos
MCP Server: Respuesta generada exitosamente con X fuentes
```

#### **Indicadores de Rendimiento:**
- **Tiempo de respuesta**: Deber√≠a ser < 5 segundos
- **N√∫mero de fuentes**: 3+ fuentes = alta confianza
- **Calidad de respuestas**: Informaci√≥n relevante y completa

### **üîí Consideraciones de Seguridad**

#### **Datos Sensibles:**
- **No incluyas informaci√≥n personal** en la base de conocimientos
- **Revisa documentos** antes de procesarlos
- **Usa fuentes confiables** para la informaci√≥n

#### **Acceso al Sistema:**
- **Mant√©n actualizado** el entorno virtual
- **Revisa logs** regularmente
- **Monitorea uso de recursos**

### **‚öôÔ∏è Optimizaci√≥n de Flujo de Trabajo**

#### **Proceso Recomendado:**
1. **Preparar documentos**: Limpiar y estructurar contenido
2. **Procesar en lotes**: Usar `bulk_ingest.py` para muchos documentos
3. **Verificar calidad**: Revisar respuestas de prueba
4. **Ajustar par√°metros**: Si es necesario, modificar configuraci√≥n
5. **Monitorear uso**: Revisar logs y rendimiento

#### **Herramientas de Verificaci√≥n:**
```bash
# Probar el sistema completo
python test_rag.py

# Verificar que Ollama funciona
ollama run llama3 "Test"

# Verificar dependencias
python -c "import mcp, langchain, chromadb; print('‚úÖ Todo OK')"
```

## **üöÄ Checklist para Funcionamiento √ìptimo**

### **Antes de Usar:**
- [ ] Ollama instalado y funcionando
- [ ] Modelo de lenguaje descargado
- [ ] Suficiente memoria RAM disponible
- [ ] Espacio en disco suficiente
- [ ] Entorno virtual activado

### **Durante el Uso:**
- [ ] Documentos bien estructurados
- [ ] Preguntas espec√≠ficas y claras
- [ ] Monitoreo de logs
- [ ] Verificaci√≥n de fuentes en respuestas
- [ ] Backup regular de datos

### **Mantenimiento:**
- [ ] Revisar logs semanalmente
- [ ] Verificar rendimiento
- [ ] Limpiar archivos temporales
- [ ] Actualizar dependencias si es necesario
- [ ] Backup de base de datos

## **‚ö†Ô∏è Problemas Comunes y Soluciones**

### **Respuestas Lentas:**
- **Causa**: Modelo muy grande o poca RAM
- **Soluci√≥n**: Usar modelo m√°s peque√±o (phi3) o aumentar RAM

### **Respuestas Pobres:**
- **Causa**: Poca informaci√≥n en la base de datos
- **Soluci√≥n**: A√±adir m√°s documentos relevantes

### **Errores de Memoria:**
- **Causa**: Documentos muy grandes o muchos fragmentos
- **Soluci√≥n**: Reducir tama√±o de fragmentos o procesar en lotes

### **B√∫squedas Sin Resultados:**
- **Causa**: Umbral de similitud muy alto
- **Soluci√≥n**: Reducir `score_threshold` a 0.6

### **Modelo No Encontrado:**
- **Causa**: Modelo no descargado o nombre incorrecto
- **Soluci√≥n**: Verificar con `ollama list` y descargar si es necesario

### **Errores de Conexi√≥n:**
- **Causa**: Ollama no est√° ejecut√°ndose
- **Soluci√≥n**: Iniciar Ollama con `ollama serve`

---

## üìÇ Estructura del Proyecto

```
/
‚îú‚îÄ‚îÄ .venv/                  # Entorno virtual de Python (creado autom√°ticamente)
‚îú‚îÄ‚îÄ rag_mcp_db/             # Base de datos vectorial (se crea al usarla)
‚îú‚îÄ‚îÄ converted_docs/         # Copias en Markdown de documentos procesados
‚îú‚îÄ‚îÄ bulk_ingest.py          # Script para la ingesta masiva desde l√≠nea de comandos
‚îú‚îÄ‚îÄ bulk_ingest_gui.py      # Script de la Interfaz Gr√°fica de Usuario
‚îú‚îÄ‚îÄ rag_core.py             # L√≥gica central y reutilizable del sistema RAG
‚îú‚îÄ‚îÄ rag_server.py           # El servidor MCP (lanzado por run_server.bat)
‚îú‚îÄ‚îÄ run_gui.bat             # Script de arranque para la interfaz gr√°fica
‚îú‚îÄ‚îÄ run_server.bat          # Script de arranque para el servidor en Windows
‚îú‚îÄ‚îÄ requirements.txt        # Todas las dependencias del proyecto
‚îú‚îÄ‚îÄ pre_download_model.py   # Script para pre-descargar el modelo de embedding
‚îú‚îÄ‚îÄ test_rag.py             # Script de prueba del sistema RAG
‚îú‚îÄ‚îÄ AGENT_INSTRUCTIONS.md   # Gu√≠a para agentes de IA
‚îî‚îÄ‚îÄ README.md               # Este archivo
```